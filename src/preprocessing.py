import re
import string
import nltk
from nltk.corpus import stopwords, wordnet
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import TweetTokenizer

# ðŸ“¥ TÃ©lÃ©charger automatiquement les ressources nÃ©cessaires
def safe_nltk_download(resource):
    try:
        nltk.data.find(f"corpora/{resource}")
    except LookupError:
        nltk.download(resource)

for res in ["stopwords", "wordnet", "omw-1.4"]:
    safe_nltk_download(res)

# ðŸ”§ Initialisation
lemmatizer = WordNetLemmatizer()
custom_stopwords = set(stopwords.words("english")) | {"rt"}

# ðŸ§¼ Fonction de nettoyage
def nettoyer_tweet(tweet):
    tweet = re.sub(r'#', '', tweet)
    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)
    tokens = tokenizer.tokenize(tweet)
    clean_tokens = [
        lemmatizer.lemmatize(word)
        for word in tokens
        if word not in custom_stopwords and word not in string.punctuation
    ]
    return " ".join(clean_tokens)
